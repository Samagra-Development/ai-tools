# -*- coding: utf-8 -*-
"""AmakrusAI

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u0ekoOYQZe19jRVHOQIDzpDRBAgdtG9F


#  0. Installs, Imports and Hugging face API Keys
"""

# RUN THIS CELL FIRST!


import os
import pandas as pd
import PyPDF2
from PyPDF2 import PdfReader
import matplotlib.pyplot as plt
from transformers import GPT2TokenizerFast
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain import HuggingFaceHub
from langchain.chains import ConversationalRetrievalChain

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "Your HuggingFace API"

"""# 1. Loading PDFs and chunking with LangChain"""

# You MUST add your PDF to local files in this notebook (folder icon on left hand side of screen)

# Simple method - Split by pages
loader = PyPDFLoader("/content/farmerbook_2.pdf")
pages = loader.load_and_split()
print(pages[0])

# SKIP TO STEP 2 IF YOU'RE USING THIS METHOD
chunks = pages

# Advanced method - Split by chunk

from transformers import AutoTokenizer

# Step 1: Convert PDF to text
import textract
doc = textract.process("/content/farmerbook_2.pdf")

# Step 2: Save to .txt and reopen (helps prevent issues)
with open('/content/farmerbook_2.pdf', 'w') as f:
    f.write(doc.decode('utf-8'))

with open('/content/farmerbook_2.pdf', 'r') as f:
    text = f.read()

# Step 3: Create function to count tokens
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

# Step 4: Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size=512,
    chunk_overlap= 24,
    length_function = count_tokens,
)

chunks = text_splitter.create_documents([text])

# Result is many LangChain 'Documents' around 500 tokens or less (Recursive splitter sometimes allows more tokens to retain context)
type(chunks[0])

# Quick data visualization to ensure chunking was successful

# Create a list of token counts
token_counts = [count_tokens(chunk.page_content) for chunk in chunks]

# Create a DataFrame from the token counts
df = pd.DataFrame({'Token Count': token_counts})

# Create a histogram of the token count distribution
df.hist(bins=40, )

# Show the plot
plt.show()

"""# 2. Embed text and store embeddings"""

# Get embedding model
embeddings = HuggingFaceEmbeddings()

# Create vector database
db = FAISS.from_documents(chunks, embeddings)

"""# 3. Setup retrieval function"""

# Check similarity search is working
query = "How to get Kisan credit cards?"
docs = db.similarity_search(query)
docs[0]

# Create QA chain to integrate similarity search with user queries (answer query from knowledge base)

llm = HuggingFaceHub(repo_id="google/flan-t5-large", model_kwargs={"temperature":0.3,"max_length":512})

chain = load_qa_chain(llm=llm, chain_type="stuff")

query = "what are the Basic Measures to Manage Weeds, Diseases and Pest Problems?"
docs = db.similarity_search(query)

chain.run(input_documents=docs, question=query,max_length=1000)

"""• Prevention is better than cure, hence follow the preventive measures discussed above
• Keep pest below ETL using preventive tech-
• Give more attention to pest warnings by Agri- culture Department officials and experts
• Be watch full on weather forecast by ICAR and niques IMD
 • Consult KVK or Scientist in early stages
  • Discuss the problems with fellow farmers, you may get many traditional tips
  • Understand the scientific rationale behind each pest management ITKs
  • Have regular contact with local Research and Development wings of agriculture and allied sectors to get a recent development. 5.10. Safety tips to reduce the risk of injuries and fatalities while handling machineries

# 5. Create chatbot with chat memory (OPTIONAL)
"""

from IPython.display import display
import ipywidgets as widgets

# Create conversation chain that uses our vectordb as retriver, this also allows for chat history management
qa = ConversationalRetrievalChain.from_llm(llm, db.as_retriever())

chat_history = []

def on_submit(_):
    query = input_box.value
    input_box.value = ""

    if query.lower() == 'exit':
        print("Thank you for using the AmakrusAI chatbot!")
        return

    result = qa({"question": query, "chat_history": chat_history})
    chat_history.append((query, result['answer']))

    display(widgets.HTML(f'<b>User:</b> {query}'))
    display(widgets.HTML(f'<b><font color="blue">Chatbot:</font></b> {result["answer"]}'))

print("Welcome to the AmakrusAI chatbot! Type 'exit' to stop.")

input_box = widgets.Text(placeholder='Please enter your question:')
input_box.on_submit(on_submit)

display(input_box)